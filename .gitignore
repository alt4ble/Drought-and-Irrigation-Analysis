from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from sklearn.metrics import confusion_matrix, classification_report
from keras.src.legacy.preprocessing.image import ImageDataGenerator
from sklearn.utils.class_weight import compute_class_weight
from keras.src.applications.densenet import DenseNet121
from keras.src.callbacks import EarlyStopping
from keras.callbacks import ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from keras.models import Model, load_model
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import requests
import zipfile
import pickle
import io
import os
from sklearn.model_selection import train_test_split
import shutil
from keras.preprocessing.image import img_to_array, load_img

# Uploading Dataset
if not os.path.exists("dataset"):
    url = "https://universe.roboflow.com/ds/Yr13ewBq6a?key=UpZncS97Tq"
    r = requests.get(url)
    z = zipfile.ZipFile(io.BytesIO(r.content))
    z.extractall("dataset")

# Combine Train and Valid
all_dir = "dataset/all_data"
if not os.path.exists(all_dir):
    os.makedirs(all_dir)
    for split in ["train", "valid"]:
        for cls in os.listdir(f"dataset/{split}"):
            src = f"dataset/{split}/{cls}"
            dst = f"{all_dir}/{cls}"
            os.makedirs(dst, exist_ok=True)
            for f in os.listdir(src):
                shutil.copy(os.path.join(src, f), dst)

# File list and Tags
dry_files = [("dry", f) for f in os.listdir(f"{all_dir}/dry")]
wet_files = [("wet", f) for f in os.listdir(f"{all_dir}/wet")]
data = dry_files + wet_files
labels = [cls for cls, _ in data]

# Split Data
train, test = train_test_split(
    data, test_size=0.15, stratify=labels, random_state=42
)
train, valid = train_test_split(
    train, test_size=0.15, stratify=[cls for cls, _ in train], random_state=42
)

# Create New Folders
base_split_dir = "dataset/split_data"
for split_name, split_data in [("train", train), ("valid", valid), ("test", test)]:
    for cls, fname in split_data:
        src = os.path.join(all_dir, cls, fname)
        dst = os.path.join(base_split_dir, split_name, cls)
        os.makedirs(dst, exist_ok=True)
        shutil.copy(src, dst)

train_dir = "dataset/split_data/train"
valid_dir = "dataset/split_data/valid"
test_dir = "dataset/split_data/test"

# ImageDataGenerator
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.3,
    brightness_range=[0.7, 1.3],
    horizontal_flip=True,
    vertical_flip=True,
    shear_range=0.2,
    channel_shift_range=20
)

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

valid_datagen = ImageDataGenerator(rescale=1./255)
valid_generator = valid_datagen.flow_from_directory(
    valid_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical'
)

test_datagen = ImageDataGenerator(rescale=1./255)
test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

augment_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.25,
    brightness_range=[0.7, 1.3],
    horizontal_flip=True,
    shear_range=0.2,
    channel_shift_range=20
)

wet_dir = os.path.join(train_dir, "wet")
augmented_wet_dir = os.path.join(train_dir, "wet")
wet_images = os.listdir(wet_dir)
print(f"Original number of wet images: {len(wet_images)}")
num_augmented = len(wet_images) // 2

i = 0
for img_name in wet_images:
    img_path = os.path.join(wet_dir, img_name)
    img = load_img(img_path, target_size=(224, 224))
    x = img_to_array(img)
    x = x.reshape((1,) + x.shape)

    for batch in augment_datagen.flow(x, batch_size=1, save_to_dir=augmented_wet_dir,
                                      save_prefix="aug", save_format="jpg"):
        i += 1
        if i >= num_augmented:
            break
    if i >= num_augmented:
        break
print(f"Number of newly produced wet images: {num_augmented}")
print(f"Total number of wet images: {len(os.listdir(wet_dir))}")

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_generator.classes),
    y=train_generator.classes
)
class_weights = dict(enumerate(class_weights))
print(class_weights)

# Model Training or Uploading
history = None
history_ft = None

if not os.path.exists("final_model.keras"):
    print("Starting to train a model")

    # --- Base Model ---
    base_model = DenseNet121(weights="imagenet",
                             include_top=False,
                             input_shape=(224, 224, 3))
    for layer in base_model.layers:
        layer.trainable = False

    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dropout(0.5)(x)
    x = Dense(256, activation="relu")(x)
    x = Dropout(0.4)(x)
    predictions = Dense(2, activation='softmax')(x)
    model = Model(inputs=base_model.input, outputs=predictions)

    model.compile(
        optimizer=Adam(learning_rate=1e-4),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5,
                                  patience=3, min_lr=1e-7, verbose=1)

    callbacks = [
        EarlyStopping(monitor='val_loss', patience=10,
                      restore_best_weights=True),
        reduce_lr
    ]

    history = model.fit(
        train_generator,
        validation_data=valid_generator,
        epochs=30,
        steps_per_epoch=len(train_generator),
        callbacks=callbacks,
        class_weight=class_weights
    )

    # Fine-Tuning
    for layer in base_model.layers[-50:]:
        layer.trainable = True

    model.compile(
        optimizer=Adam(learning_rate=1e-5),
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    callbacks_ft = [
        EarlyStopping(monitor='val_loss', patience=10,
                      restore_best_weights=True),
        reduce_lr
    ]

    history_ft = model.fit(
        train_generator,
        validation_data=valid_generator,
        epochs=30,
        callbacks=callbacks_ft
    )

    model.save("final_model.keras")
    print("Training completed. Model saved as final_model.keras")

    with open("history.pkl", "wb") as f:
        pickle.dump((history.history, history_ft.history), f)
else:
    model = load_model("final_model.keras")
    print("Training skipped, model loaded as final_model.keras")

    if os.path.exists("history.pkl"):
        with open("history.pkl", "rb") as f:
            h1, h2 = pickle.load(f)
        history = type("History", (), {"history": h1})()
        history_ft = type("History", (), {"history": h2})()

test_loss, test_accuracy = model.evaluate(test_generator)
print(f"\nTest Accuracy: {test_accuracy:.2f}")

Y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(Y_pred, axis=1)
y_true = test_generator.classes

class_indices_sorted = sorted(test_generator.class_indices.items(), key=lambda item: item[1])
class_labels = [class_name for class_name, _ in class_indices_sorted]

print("\nClassification Report:")
print(classification_report(y_true, y_pred_classes, target_names=class_labels))

# Confusion Matrix
y_true_labels = [class_labels[i] for i in y_true]
y_pred_labels = [class_labels[i] for i in y_pred_classes]
cm = confusion_matrix(y_true_labels, y_pred_labels, labels=class_labels)
cm_rotated = cm[::-1, :]
plt.figure(figsize=(6, 6))
sns.heatmap(cm_rotated, annot=True, fmt="d", cmap="Blues",
            xticklabels=[f"{c}" for c in class_labels],
            yticklabels=[f"{c}" for c in class_labels[::-1]],
            cbar=False, square=True)
plt.title("Confusion Matrix")
plt.ylabel("True Label")
plt.xlabel("Predicted Label")
plt.show()

# Accuracy Graphs
if history is not None and history_ft is not None:
    train_acc = history.history['accuracy'] + history_ft.history['accuracy']
    val_acc = history.history['val_accuracy'] + history_ft.history['val_accuracy']
    plt.plot(train_acc, label='Train Accuracy')
    plt.plot(val_acc, label='Valid Accuracy')
    plt.title('Model Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

    train_loss = history.history['loss'] + history_ft.history['loss']
    val_loss = history.history['val_loss'] + history_ft.history['val_loss']
    plt.plot(train_loss, label='Train Loss')
    plt.plot(val_loss, label='Valid Loss')
    plt.title('Model Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()
